{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "* 呼叫 `get_AML_person(content, ckip_name, mode=0, binary=0)` 即可\n",
    "    * `mode=0`, `binary` 無作用；使用模型預測 binary\n",
    "    * `mode=1`, `binary` 用來放其他模型的 binary 分類輸出 (int 1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "from zhon.hanzi import stops, non_stops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "\n",
    "import datetime #####\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_NAME = 'bert_wwm_pretrain_tbrain' # pretrained_bert_wwm\n",
    "MODEL_PATH = './model/pre_bert_wwm_bio_only_EPOCHES_9(w359).pkl'\n",
    "\n",
    "def clean_string(content):\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
    "    content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "    result = []\n",
    "    for i in range(math.ceil(len(content) / 511)):\n",
    "        result.append(content[i*512 : i*512+511])\n",
    "    return result\n",
    "\n",
    "def bio_2_string(have_AML, BIO_tagging, ckip_result, origin_text, BIO_prob):\n",
    "  result = []\n",
    "  if (have_AML.item() == 0):\n",
    "    result.append('')\n",
    "  else:\n",
    "\"\"\"一個span找一次名字(慢很多)\"\"\"\n",
    "#     for j in range(1, 512):\n",
    "#       if (BIO_tagging[j] == 0):\n",
    "#         start = j\n",
    "#         end = j + 1\n",
    "#         while (end < 512 and BIO_tagging[end] == 1):\n",
    "#           end += 1\n",
    "#         if (end > start + 1):\n",
    "#           if (start <= 3):\n",
    "#               s = origin_text[start-1 : end + 2] # -1 for CLS\n",
    "# #               print(BIO_prob[start : end + 3])\n",
    "#           else:\n",
    "#               s = origin_text[start-1-1 : end + 2] # -1 for CLS\n",
    "# #               print(BIO_prob[start-1 : end + 3])\n",
    "# #           print('origin_span: ', origin_text[start-1 : end-1])\n",
    "# #           print(s)\n",
    "#           for k in range(len(ckip_result)):\n",
    "#             if (len(ckip_result[k]) < 2):\n",
    "#               continue\n",
    "#             elif (re.findall(r\"[%s]+\" %non_stops, ckip_result[k]) != [] \\\n",
    "#                      or re.findall(r\"[%s]+\" %stops, ckip_result[k]) != []): # 有標點\n",
    "#               continue\n",
    "#             found = s.find(ckip_result[k])\n",
    "#             if (found != -1):\n",
    "# #               print('found: ', found)\n",
    "#               result.append(ckip_result[k])\n",
    "\n",
    "\"\"\"把span串在一起找名字(比較快)\"\"\"\n",
    "    full_str = \"\"\n",
    "    for j in range(1, 512):\n",
    "      if (BIO_tagging[j] == 0):\n",
    "        start = j\n",
    "        end = j + 1\n",
    "        while (end < 512 and BIO_tagging[end] == 1):\n",
    "          end += 1\n",
    "        if (end > start + 1):\n",
    "          if (start <= 3):\n",
    "              s = origin_text[start-1 : end + 2] # -1 for CLS\n",
    "#               print(BIO_prob[start : end + 3])\n",
    "          else:\n",
    "              s = origin_text[start-1-1 : end + 2] # -1 for CLS\n",
    "#               print(BIO_prob[start-1 : end + 3])\n",
    "          print('origin_span: ', origin_text[start-1 : end-1])\n",
    "          print(s)\n",
    "          full_str += s\n",
    "    for k in range(len(ckip_result)):\n",
    "      if (len(ckip_result[k]) < 2):\n",
    "        continue\n",
    "      elif (re.findall(r\"[%s]+\" %non_stops, ckip_result[k]) != [] \\\n",
    "                 or re.findall(r\"[%s]+\" %stops, ckip_result[k]) != []): # 有標點\n",
    "        continue\n",
    "      found = full_str.find(ckip_result[k])\n",
    "      if (found != -1):\n",
    "#       print('found: ', found)\n",
    "        result.append(ckip_result[k])\n",
    "      \n",
    "    if (len(result) == 0):\n",
    "      result.append('')\n",
    "  return result\n",
    "\n",
    "def get_predictions(model, tokens_tensors, segments_tensors, masks_tensors, ckip_names, origin_text, mode, binary):\n",
    "  result = []\n",
    "  with torch.no_grad():\n",
    "      tokens_tensors = tokens_tensors.to(\"cuda:0\")\n",
    "      segments_tensors = segments_tensors.to(\"cuda:0\")\n",
    "      masks_tensors = masks_tensors.to(\"cuda:0\")\n",
    "#       start = datetime.datetime.now().timestamp() ######\n",
    "      outputs = model(input_ids=tokens_tensors, \n",
    "                  token_type_ids=segments_tensors, \n",
    "                  attention_mask=masks_tensors)\n",
    "#       end = datetime.datetime.now().timestamp()###########\n",
    "#       print(\"through model time: \", end-start) ##########\n",
    "      \n",
    "      count = outputs[0].shape[0]\n",
    "      for i in range(count):  # run batchsize times\n",
    "        if (mode == 0):\n",
    "            have_AML = outputs[0][i].argmax()\n",
    "        else:\n",
    "            have_AML = torch.tensor([binary])\n",
    "        BIO_pred = outputs[0][i].argmax(1) # 3*512 into class label\n",
    "        ckip_names_list = ast.literal_eval(ckip_names) # string to list\n",
    "#         print(origin_text[i])\n",
    "#         start = datetime.datetime.now().timestamp() ######\n",
    "        r = bio_2_string(have_AML, BIO_pred, ckip_names_list, origin_text[i], outputs[0][i])  #####\n",
    "#         end = datetime.datetime.now().timestamp()###########\n",
    "#         print(\"bio_2_string time: \", end-start) ##########\n",
    "        result.append(r)\n",
    "  return result\n",
    "\n",
    "\"\"\" model budling \"\"\"\n",
    "class AMLPredictModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AMLPredictModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRETRAINED_MODEL_NAME, config = config)\n",
    "        self.BIO_classifier = nn.Sequential(\n",
    "                        nn.Linear(config.hidden_size, 3),\n",
    "        ) # BIO tagging\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        BIO = self.BIO_classifier(outputs[0]) # 512*HIDDENSIZE word vectors\n",
    "        BIO = self.softmax(BIO)\n",
    "        \n",
    "        outputs = (BIO,) + outputs[2:]\n",
    "        return outputs\n",
    "    \n",
    "config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME, output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model = AMLPredictModel(config)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    \n",
    "def get_AML_person(model, content, ckip_name, mode=0, binary=0):\n",
    "#     start = datetime.datetime.now().timestamp() ######\n",
    "    content = clean_string(content)\n",
    "#     end = datetime.datetime.now().timestamp()###########\n",
    "#     print(\"clean_string time: \", end-start) ##########\n",
    "#     start = datetime.datetime.now().timestamp() #####\n",
    "    test_input_dict = tokenizer.batch_encode_plus(content, \n",
    "                          add_special_tokens=True,\n",
    "                          max_length=512,\n",
    "                          return_special_tokens_mask=True,\n",
    "                          pad_to_max_length=True,\n",
    "                          return_tensors='pt',\n",
    "                          truncation=True)\n",
    "#     end = datetime.datetime.now().timestamp()###########\n",
    "#     print(\"tokenizer time: \", end-start) ##########\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    r = get_predictions(model, test_input_dict['input_ids'], test_input_dict['token_type_ids'], test_input_dict['attention_mask'],\\\n",
    "                           ckip_name, content, mode, binary)\n",
    "#     print(result)\n",
    "    result = set()\n",
    "    for i in range(len(r)):\n",
    "        result = result | set(r[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## name classifier (QA model) by Houg Yun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "name classifier (QA model) by Houg Yun\n",
    "input: predicted name (list), news(text), dataset num(int)\n",
    "output: predict name (list)\n",
    "model has been delete QAQ\n",
    "\"\"\"\n",
    "from transformers import BertForSequenceClassification\n",
    "def qa_name_binary_ensemble(pred_name_list, news):\n",
    "    num_labels = 2\n",
    "    lm_path = './chinese_roberta_wwm/'\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model_path = '../../nlp/NewsClassify/QAModel/1/'\n",
    "    rbt0_checkpoint = model_path + 'roberta_init0_all_data_name_qa_split_epoch0.pkl'\n",
    "    rbt1_checkpoint = model_path + 'roberta_init1_all_data_name_qa_split_epoch2.pkl'\n",
    "    rbt2_checkpoint = model_path + 'roberta_init2_all_data_name_qa_split_epoch2.pkl'\n",
    "\n",
    "    model0 = BertForSequenceClassification.from_pretrained(lm_path,num_labels=num_labels)\n",
    "    model0.load_state_dict(torch.load(rbt0_checkpoint))\n",
    "    model0.to(device)\n",
    "    model0.eval()\n",
    "    \n",
    "    model1 = BertForSequenceClassification.from_pretrained(lm_path,num_labels=num_labels)\n",
    "    model1.load_state_dict(torch.load(rbt1_checkpoint))\n",
    "    model1.to(device)\n",
    "    model1.eval()\n",
    "    \n",
    "    model2 = BertForSequenceClassification.from_pretrained(lm_path,num_labels=num_labels)\n",
    "    model2.load_state_dict(torch.load(rbt2_checkpoint))\n",
    "    model2.to(device)\n",
    "    model2.eval()\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "    def clean_string(content):\n",
    "        content = content.replace('\\n','').replace('\\t','').replace(' ','').replace('\\xa0','')\n",
    "        content = re.sub(\"[●▼►★]\", \"\",content)\n",
    "        return content\n",
    "\n",
    "    def cut_sent(para):\n",
    "        para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)\n",
    "        para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para) \n",
    "        para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  \n",
    "        para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "        return para.split(\"\\n\")\n",
    "    \n",
    "    class Testset(Dataset):\n",
    "        def __init__(self, input_ids, token_type_ids, attention_mask, names):\n",
    "            self.input_ids = input_ids\n",
    "            self.token_type_ids = token_type_ids\n",
    "            self.attention_mask = attention_mask\n",
    "            self.names = names\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "            name = self.names[idx]\n",
    "            return inputid, tokentype, attentionmask, name\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "\n",
    "    def combine_sentence(sentences, max_len):\n",
    "        li = []\n",
    "        string = \"\"\n",
    "        for k in range(len(sentences)):\n",
    "            sentence = sentences[k]\n",
    "            if len(string) + len(sentence) < max_len:\n",
    "                string = string + sentence\n",
    "            else:\n",
    "                #             原本是空的代表sentences太常\n",
    "                if string == \"\":\n",
    "                    n = max_len\n",
    "                    tmp_li = [sentence[i : i + n] for i in range(0, len(sentence), n)]\n",
    "                    string = tmp_li.pop(-1)\n",
    "                    li = li + tmp_li\n",
    "                else:\n",
    "                    li.append(string)\n",
    "                    string = sentence\n",
    "        if string != \"\":\n",
    "            li.append(string)\n",
    "        return li\n",
    "\n",
    "    train_input_ids = []\n",
    "    train_token_types = []\n",
    "    train_attention_mask = []\n",
    "    testing_name = []\n",
    "\n",
    "    content = clean_string(news)\n",
    "\n",
    "    max_length = 500\n",
    "\n",
    "    split_content = cut_sent(content)\n",
    "    chunks = combine_sentence(split_content, max_length)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        for name in pred_name_list:\n",
    "            if len(chunk) >= max_length:\n",
    "                print(\"error !!!! lenth > 500\")\n",
    "                continue\n",
    "            if name not in chunk:\n",
    "                continue\n",
    "\n",
    "            input_ids = tokenizer.encode(name, chunk)\n",
    "            if len(input_ids) > 512:\n",
    "                continue\n",
    "            sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "            num_seg_a = sep_index + 1\n",
    "            num_seg_b = len(input_ids) - num_seg_a\n",
    "            segment_ids = [0] * num_seg_a + [1] * num_seg_b\n",
    "\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < 512:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            train_input_ids.append(input_ids)\n",
    "            train_token_types.append(segment_ids)\n",
    "            train_attention_mask.append(input_mask)\n",
    "            testing_name.append(name)\n",
    "\n",
    "    train_input_ids = np.array(train_input_ids)\n",
    "    train_token_types = np.array(train_token_types)\n",
    "    train_attention_mask = np.array(train_attention_mask)\n",
    "    testing_name = np.array(testing_name)\n",
    "\n",
    "    BATCH_SIZE = train_input_ids.shape[0]\n",
    "    testset = Testset(\n",
    "        train_input_ids, train_token_types, train_attention_mask, testing_name\n",
    "    )\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors = [\n",
    "                t.to(device) for t in data[:-1]\n",
    "            ]\n",
    "            name = data[-1]\n",
    "            pred_name_list = np.array(name)\n",
    "            \n",
    "            outputs0 = model0(\n",
    "                input_ids=tokens_tensors,\n",
    "                token_type_ids=segments_tensors,\n",
    "                attention_mask=masks_tensors,\n",
    "            )\n",
    "            pred0 = torch.softmax(outputs0[0], dim=-1)\n",
    "            pred0 = torch.argmax(pred0, dim=-1)\n",
    "            pred0 = pred0.cpu().detach().numpy()\n",
    "            ans0 = list(pred_name_list[pred0 > 0])\n",
    "            \n",
    "            outputs1 = model1(\n",
    "                input_ids=tokens_tensors,\n",
    "                token_type_ids=segments_tensors,\n",
    "                attention_mask=masks_tensors,\n",
    "            )\n",
    "            pred1 = torch.softmax(outputs1[0], dim=-1)\n",
    "            pred1 = torch.argmax(pred1, dim=-1)\n",
    "            pred1 = pred1.cpu().detach().numpy()\n",
    "            ans1 = list(pred_name_list[pred1 > 0])\n",
    "            \n",
    "            \n",
    "            outputs2 = model2(\n",
    "                input_ids=tokens_tensors,\n",
    "                token_type_ids=segments_tensors,\n",
    "                attention_mask=masks_tensors,\n",
    "            )\n",
    "            pred2 = torch.softmax(outputs2[0], dim=-1)\n",
    "            pred2 = torch.argmax(pred2, dim=-1)\n",
    "            pred2 = pred2.cpu().detach().numpy()\n",
    "            ans2 = list(pred_name_list[pred2 > 0])\n",
    "            \n",
    "            \n",
    "            vote_result = []\n",
    "            for name in list(set(ans0 + ans1 + ans2)):\n",
    "                vote = 0\n",
    "                vote += name in ans0\n",
    "                vote += name in ans1\n",
    "                vote += name in ans2\n",
    "                if vote >=2:\n",
    "                    vote_result.append(name)\n",
    "\n",
    "            \n",
    "            return vote_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## single test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '台北地檢署偵辦台北市警中山分局中山一派出所集體貪瀆弊案，上月初判中山一派出所前、後3任所長林子芸、楊修白、劉靜怡、林子芸妻子林思賢、行賄「白手套」黃念心5人無罪，台北地檢署不服判決，對5人提起上訴。  檢調偵辦台北市中山區「立邦」酒店媒介外籍女子賣淫案，查出林子芸等官警自2004年起至2017年按月收業者4萬元賄款，以不臨檢、通風報信方式包庇酒店經營；「夜王」酒店從2007年起到2017年每月向派出所員警行賄1萬5000元，將涉案員警、業者多人起訴。  台北地院審理，上月6日將員警李石良判刑16年、判游怡如13年、判紀宏白8年、判張佳雯14年、判紀炳場14年、判陳宏洲10年10月、判莊琦良12年半，另員警曾學函、楊惠志、蔣盈君獲判緩刑，不過，林子芸、楊修白、劉靜怡、林思賢、黃念心5人獲判無罪。  台北地檢署檢察官收判後，認為法官判決林子芸等5人無罪的理由違反經驗法則、論理法則，日前向台灣高等法院提起上訴'\n",
    "ckip_n = \"['莊琦良', '黃念心', '判紀宏白', '張佳雯', '楊修白', '林思賢', '判紀炳場', '林子芸', '曾學函', '劉靜怡', '蔣盈君', '楊惠志', '陳宏洲', '游怡如', '李石良']\"\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now().timestamp()\n",
    "ans = get_AML_person(model, t, ckip_n, mode=1, binary=1)\n",
    "end = datetime.datetime.now().timestamp()\n",
    "print('ans: ', ans)\n",
    "print('total time: ', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## multi test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/2020-07-29.csv')\n",
    "all_ans = []\n",
    "for i in range(df.shape[0]):\n",
    "    t = df.loc[i, 'article']\n",
    "    ckip_n = df.loc[i, 'ckip_name']\n",
    "    start = datetime.datetime.now().timestamp()\n",
    "    b = df.loc[i, 'binary']\n",
    "    ans = get_AML_person(model, t, ckip_n, mode=1, binary=b)\n",
    "    end = datetime.datetime.now().timestamp()\n",
    "#     print('ans: ', ans)\n",
    "#     print('total time: ', end-start)\n",
    "    all_ans.append(list(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(all_ans)):\n",
    "    if (all_ans[i][0] == '' and len(all_ans[i]) == 1 or len(all_ans[i]) == 0):\n",
    "        result.append([''])\n",
    "        continue\n",
    "    result.append(qa_name_binary_ensemble(all_ans[i] , df.loc[i,'article']))\n",
    "dict = {'BIO_ans' : all_ans, 'QA_ans' : result}\n",
    "df_ans = pd.DataFrame(dict)\n",
    "df_ans = pd.concat([df['predict_name'], df_ans], axis = 1)\n",
    "df_ans.to_csv('0729_pred_n.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
