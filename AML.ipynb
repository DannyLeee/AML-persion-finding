{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mf3EflIBRILC"
   },
   "source": [
    "# AML Finding\n",
    "## Data Preprocessing\n",
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1595406344518,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "f47gpSwLRILD",
    "outputId": "ddc5035c-bf71-4094-964f-824cd70eb731",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4917, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "from zhon.hanzi import stops, non_stops\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# PRETRAINED_MODEL_NAME = \"hfl/rbtl3\" # RBTL3\n",
    "PRETRAINED_MODEL_NAME = \"bert_wwm_pretrain_tbrain\" # pretrained_bert_wwm\n",
    "df_train = pd.read_csv('./dataset/tbrain_train_0.csv')\n",
    "df_2 = pd.read_csv('./dataset/tbrain_test_0.csv')\n",
    "df_train = pd.concat([df_train, df_2])\n",
    "df_train = df_train.fillna('[\\'\\]')\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_ID</th>\n",
       "      <th>name</th>\n",
       "      <th>full_content</th>\n",
       "      <th>ckip_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3616</td>\n",
       "      <td>[]</td>\n",
       "      <td>為加快地方政府專項債券發行使用進度，中國財政部今 (27 日) 提前宣布 2020 年部分專...</td>\n",
       "      <td>['花長春', '唐建偉']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4208</td>\n",
       "      <td>[]</td>\n",
       "      <td>勞動部昨天發布「勞動契約認定指導原則」，盼供勞雇雙方檢視勞動契約關係，勞動部長許銘春今天表示...</td>\n",
       "      <td>['銘春', '許銘春']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3444</td>\n",
       "      <td>[]</td>\n",
       "      <td>48年次的勞工注意！依照勞工保險條例規定，從明年開始，勞保的老年年金法定請領年齡將二度調高，...</td>\n",
       "      <td>['陳']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3328</td>\n",
       "      <td>[]</td>\n",
       "      <td>過去曾在馬雷蒙舞群中擔任舞者的男子施政雄，離開舞群轉行成為新北市「錢莊大亨」，放款在外的金額...</td>\n",
       "      <td>['陳天賜', '唐世豪', '陳柔瑜', '唐', '施男', '江', '馬雷蒙', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2617</td>\n",
       "      <td>[]</td>\n",
       "      <td>【19:25】特首林鄭月娥會同行政會議，通過引用《緊急法》訂立《禁蒙面法》（《禁止蒙面規例》...</td>\n",
       "      <td>['敖暉', '林鄭月娥', '岑敖暉', '郭卓堅']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_ID name                                       full_content  \\\n",
       "0     3616   []  為加快地方政府專項債券發行使用進度，中國財政部今 (27 日) 提前宣布 2020 年部分專...   \n",
       "1     4208   []  勞動部昨天發布「勞動契約認定指導原則」，盼供勞雇雙方檢視勞動契約關係，勞動部長許銘春今天表示...   \n",
       "2     3444   []  48年次的勞工注意！依照勞工保險條例規定，從明年開始，勞保的老年年金法定請領年齡將二度調高，...   \n",
       "3     3328   []  過去曾在馬雷蒙舞群中擔任舞者的男子施政雄，離開舞群轉行成為新北市「錢莊大亨」，放款在外的金額...   \n",
       "4     2617   []  【19:25】特首林鄭月娥會同行政會議，通過引用《緊急法》訂立《禁蒙面法》（《禁止蒙面規例》...   \n",
       "\n",
       "                                          ckip_names  \n",
       "0                                     ['花長春', '唐建偉']  \n",
       "1                                      ['銘春', '許銘春']  \n",
       "2                                              ['陳']  \n",
       "3  ['陳天賜', '唐世豪', '陳柔瑜', '唐', '施男', '江', '馬雷蒙', '...  \n",
       "4                       ['敖暉', '林鄭月娥', '岑敖暉', '郭卓堅']  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOO0iT-SRILK"
   },
   "outputs": [],
   "source": [
    "def clean_string(content):\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
    "    content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCS6r7fQRILO"
   },
   "outputs": [],
   "source": [
    "def find_all(name, content):\n",
    "    # +1 for [CLS]\n",
    "    pos_list = [m.start()+1 for m in re.finditer(name, content)]\n",
    "    count = len(pos_list)\n",
    "    return pos_list , count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzAZ4mBzRILS"
   },
   "outputs": [],
   "source": [
    "def orgi_2_array(names, contents):\n",
    "    x = []\n",
    "    binary_y = []\n",
    "    BIO_labels = []\n",
    "    nFound_count = 0\n",
    "    name_count = 0\n",
    "    \n",
    "    for i in range(len(contents)):\n",
    "        content = contents[i]\n",
    "        content = clean_string(content)\n",
    "\n",
    "        # record names\n",
    "        # name = names[i] # single\n",
    "        name_list = names[i]\n",
    "        names_label = ast.literal_eval(name_list) # string to list\n",
    "        # debug\n",
    "        \n",
    "\n",
    "        # init pos label arr\n",
    "        BIO_label = np.full((512), 2) # initial to all 2 (outside)\n",
    "        \n",
    "        # no AML person\n",
    "        if(name_list == '[]'):\n",
    "            binary_y.append(0)\n",
    "            x.append(content)\n",
    "            BIO_label[0] = 0 # first position 0(begin)\n",
    "            BIO_labels.append(BIO_label)\n",
    "\n",
    "        else:\n",
    "            # initial position list\n",
    "            start_pos = []\n",
    "            end_pos = []\n",
    "\n",
    "            # if (True): # single\n",
    "            for name in names_label:\n",
    "              temp, count = find_all(name, content)\n",
    "              if(temp == []):\n",
    "  #                 print(name + ' find error in data', i)\n",
    "                  nFound_count += 1\n",
    "                  continue\n",
    "              for j in range(count):\n",
    "                start_pos.append(temp[j])\n",
    "                end_pos.append(temp[j] + len(name))\n",
    "\n",
    "#                  01234\n",
    "#                B 00100\n",
    "#                I 00011\n",
    "#                O 11000\n",
    "            for j in range(len(start_pos)):\n",
    "                if(start_pos[j] < 512 and end_pos[j] < 512):\n",
    "                    BIO_label[start_pos[j]] = 0\n",
    "                    BIO_label[start_pos[j]+1 : end_pos[j]] = 1\n",
    "            binary_y.append(1)\n",
    "            x.append(content)\n",
    "            BIO_labels.append(BIO_label)\n",
    "            \n",
    "\n",
    "    x = np.array(x)\n",
    "    binary_y = np.array(binary_y)\n",
    "    BIO_labels = np.array(BIO_labels)\n",
    "    \n",
    "    print('nFound: ', nFound_count)\n",
    "    print('name_count:', name_count)\n",
    "    print(x.shape)\n",
    "    print(binary_y.shape)\n",
    "#     print(begin_pos_labels.shape)\n",
    "#     print(inside_pos_labels.shape)\n",
    "#     print(outside_pos_labels.shape)\n",
    "    print(BIO_labels.shape)\n",
    "    return x, binary_y, BIO_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNbG1YCCocSR"
   },
   "source": [
    "### Get Data List (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3849,
     "status": "ok",
     "timestamp": 1595406353080,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "xgxsXLjgRILW",
    "outputId": "a2d6345c-b3d6-4477-fdfe-52695ac6ef00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nFound:  0\n",
      "name_count: 0\n",
      "(4917,)\n",
      "(4917,)\n",
      "(4917, 512)\n"
     ]
    }
   ],
   "source": [
    "names =  df_train['name']\n",
    "contents = np.array(df_train['full_content'].tolist())\n",
    "train_x, train_binary_y, train_bio_labels = orgi_2_array(names, contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3691,
     "status": "ok",
     "timestamp": 1595406353081,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "f2KfiQ36RILa",
    "outputId": "5035baf2-58ff-4739-e118-889dcfe0e7de",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4917 4917\n",
      "0.07565588773642465\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x),len(train_binary_y))\n",
    "print(sum(train_binary_y)/len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6962 4885 2505657\n",
      "359.9047687446136 512.9287615148413 1.0\n"
     ]
    }
   ],
   "source": [
    "c_0, c_1, c_2 = 0, 0, 0\n",
    "for i in range(len(train_bio_labels)):\n",
    "    for j in range(512):\n",
    "        if (train_bio_labels[i][j] == 0):\n",
    "            c_0 +=1\n",
    "        elif (train_bio_labels[i][j] == 1):\n",
    "            c_1 += 1\n",
    "        else:\n",
    "            c_2 += 1\n",
    "print(c_0, c_1, c_2)\n",
    "print(c_2 / c_0, c_2 / c_1, c_2 / c_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bio_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_prpyiUxRILf"
   },
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDGrE3spRILg"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, y , bio_labels):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.y = y\n",
    "        self.bio_labels = bio_labels\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        bio_label = self.bio_labels[idx]\n",
    "        y = self.y[idx]\n",
    "        return inputid , tokentype , attentionmask, y , bio_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_dict):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        return inputid , tokentype , attentionmask, \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0TjewOuRILk"
   },
   "source": [
    "### Go Through Tokenizer (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PR3fIw2KqesN"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "# 把input轉換成bert格式\n",
    "train_input_dict = tokenizer.batch_encode_plus(train_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 4158, 1217,  ...,    0,    0,    0],\n",
       "        [ 101, 1246, 1240,  ..., 1246, 2339,  102],\n",
       "        [ 101, 8214, 2399,  ..., 2399, 2399,  102],\n",
       "        ...,\n",
       "        [ 101, 7676, 3949,  ..., 2697, 4638,  102],\n",
       "        [ 101, 9160, 9131,  ..., 6303, 8024,  102],\n",
       "        [ 101,  523, 8124,  ...,  519, 2571,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'special_tokens_mask': tensor([[1, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 0, 0,  ..., 0, 0, 1],\n",
       "        [1, 0, 0,  ..., 0, 0, 1],\n",
       "        ...,\n",
       "        [1, 0, 0,  ..., 0, 0, 1],\n",
       "        [1, 0, 0,  ..., 0, 0, 1],\n",
       "        [1, 0, 0,  ..., 0, 0, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntuaz0TXRILp"
   },
   "source": [
    "## Model Budling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9gzlkNxRILq"
   },
   "outputs": [],
   "source": [
    "\"\"\" model budling \"\"\"\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AMLPredictModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AMLPredictModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRETRAINED_MODEL_NAME, config = config)\n",
    "        self.BIO_classifier = nn.Sequential(\n",
    "                        nn.Linear(config.hidden_size, 3),\n",
    "        ) # BIO tagging\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "#         position_ids=None,\n",
    "#         head_mask=None,\n",
    "#         inputs_embeds=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "#             position_ids=position_ids,\n",
    "#             head_mask=head_mask,\n",
    "#             inputs_embeds=inputs_embeds\n",
    "        )\n",
    "#         have_AML = outputs[1] # pooled cls (cls token through 1 linear and tanh)\n",
    "#         have_AML = self.classifier(have_AML)\n",
    "        \n",
    "        BIO = self.BIO_classifier(outputs[0]) # 512*HIDDENSIZE word vectors\n",
    "        BIO = self.softmax(BIO)\n",
    "        \n",
    "        outputs = (BIO, ) + outputs[2:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mvE2_BOo2aP"
   },
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3882,
     "status": "ok",
     "timestamp": 1595406443519,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "MnV2_agYRILu",
    "outputId": "65a6c2b6-952b-42de-b254-65431a43dc7e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "BIO_classifier  Sequential(\n",
      "  (0): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n",
      "softmax         Softmax(dim=-1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" model setting (training)\"\"\"\n",
    "from transformers import BertConfig, AdamW\n",
    "config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME, output_hidden_states=True)\n",
    "BATCH_SIZE = 4\n",
    "trainSet = TrainDataset(train_input_dict, train_binary_y, train_bio_labels)\n",
    "trainLoader = DataLoader(trainSet, batch_size=BATCH_SIZE)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = AMLPredictModel(config)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5) # AdamW = BertAdam\n",
    "binary_loss_fct = nn.CrossEntropyLoss()\n",
    "weight = torch.FloatTensor([359, 510, 1]).cuda()\n",
    "# 1000 900 1 ()被蓋掉\n",
    "# 359 510 1\n",
    "# 500 450 1\n",
    "# 250 150 1 (X)\n",
    "# 125 50 1 (X)\n",
    "BIO_loss_fct = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "#             print(_)\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 626074,
     "status": "ok",
     "timestamp": 1595407176170,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "N-Uw9qeDRIL1",
    "outputId": "ca596dee-eec6-46ae-a9b3-7d84d7cebfd4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-04 19:42:06.624851+08:00\n",
      "2020-08-04 19:45:59.838005+08:00\t[epoch 1] loss: 791.557\n",
      "2020-08-04 19:49:55.869613+08:00\t[epoch 2] loss: 755.746\n",
      "2020-08-04 19:53:51.587802+08:00\t[epoch 3] loss: 747.321\n",
      "2020-08-04 19:57:47.388525+08:00\t[epoch 4] loss: 747.447\n",
      "2020-08-04 20:01:43.383353+08:00\t[epoch 5] loss: 746.091\n",
      "2020-08-04 20:05:39.401393+08:00\t[epoch 6] loss: 746.744\n",
      "2020-08-04 20:09:35.809258+08:00\t[epoch 7] loss: 744.527\n",
      "2020-08-04 20:13:31.633817+08:00\t[epoch 8] loss: 742.167\n",
      "2020-08-04 20:17:28.052410+08:00\t[epoch 9] loss: 745.524\n",
      "2020-08-04 20:21:24.501304+08:00\t[epoch 10] loss: 742.552\n"
     ]
    }
   ],
   "source": [
    "\"\"\" training \"\"\"\n",
    "from datetime import datetime,timezone,timedelta\n",
    "\n",
    "model = model.to(device)\n",
    "model.train() ##########################\n",
    "\n",
    "EPOCHS = 10\n",
    "dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "print(dt2)\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    binary_running_loss = 0.0\n",
    "    BIO_running_loss = 0.0\n",
    "    for data in trainLoader:\n",
    "    # data = testSet[21] # test model\n",
    "    # if(True):\n",
    "        \n",
    "      tokens_tensors, segments_tensors, masks_tensors, \\\n",
    "      labels, BIO_label = [t.to(device) for t in data]\n",
    "\n",
    "      # 將參數梯度歸零\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # forward pass\n",
    "      outputs = model(input_ids=tokens_tensors, \n",
    "                      token_type_ids=segments_tensors, \n",
    "                      attention_mask=masks_tensors)\n",
    "\n",
    "      BIO_pred = outputs[0]\n",
    "      BIO_pred = torch.transpose(BIO_pred, 1, 2)\n",
    "\n",
    "      # print(BIO_pred.shape)\n",
    "      # print(BIO_label.shape)\n",
    "      BIO_loss = BIO_loss_fct(BIO_pred, BIO_label)\n",
    "      # print(binary_loss, BIO_loss)\n",
    "      loss = BIO_loss\n",
    "      # print(loss)\n",
    "      # break\n",
    "      \n",
    "      # backward\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # 紀錄當前 batch loss\n",
    "      running_loss += loss.item()\n",
    "      BIO_running_loss += BIO_loss.item()\n",
    "        \n",
    "    CHECKPOINT_NAME = './model_final_state_dict/pre_bert_wwm_bio_only_EPOCHES_' + str(epoch) + '(w359).pkl' \n",
    "    torch.save(model.state_dict(), CHECKPOINT_NAME)\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    # _, binary_acc, bio_acc = get_predictions(model, trainLoader, compute_acc=True)\n",
    "    dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "    dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "    print('%s\\t[epoch %d] loss: %.3f' %\n",
    "          (dt2, epoch + 1, running_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WSwVmiBRIL5"
   },
   "source": [
    "---\n",
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1595407694683,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "XnTi_GkuEvxu",
    "outputId": "2a4168be-6c18-48ae-b7ed-93e5df188ccb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(491, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "from zhon.hanzi import stops, non_stops\n",
    "# df_test = pd.read_csv('./dataset/multi_tbrain_test.csv')\n",
    "df_test = pd.read_csv('./dataset/tbrain_test_0.csv')\n",
    "\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data List (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1595407696907,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "jGmYmQWSpChJ",
    "outputId": "6f823e5c-9c7f-43b6-d235-12b6c7ebaf6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nFound:  0\n",
      "name_count: 0\n",
      "(491,)\n",
      "(491,)\n",
      "(491, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names =  df_test['name']\n",
    "contents = np.array(df_test['full_content'].tolist())\n",
    "test_x, test_binary_y, test_bio_labels = orgi_2_array(names, contents)\n",
    "test_binary_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lBWBRl9rw4D"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# PRETRAINED_MODEL_NAME = \"hfl/rbtl3\" # RBTL3\n",
    "PRETRAINED_MODEL_NAME = \"bert_wwm_pretrain_tbrain\" # pretrained_bert_wwm\n",
    "# MODEL_PATH = './model_3/RBTL3_bio_only_EPOCHES_9.pkl'\n",
    "MODEL_PATH = './model/pre_bert_wwm_bio_only_EPOCHES_9(w359).pkl'\n",
    "# MODEL_PATH = './model/pre_bert_wwm_bio_only_EPOCHES_9.pkl'\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go Through Tokenizer (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HTrUZ6Ct98e"
   },
   "outputs": [],
   "source": [
    "test_input_dict = tokenizer.batch_encode_plus(test_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Decode Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5zN9utmvc19"
   },
   "outputs": [],
   "source": [
    "def bio_2_string(tokens_tensors, have_AML, BIO_tagging, ckip_result, BIO_prob):\n",
    "  result = []\n",
    "  if (have_AML.item() == 0):\n",
    "    result.append('')\n",
    "  else:\n",
    "    for j in range(1, 512):\n",
    "      if (BIO_tagging[j] == 0):\n",
    "        start = j\n",
    "        end = j + 1\n",
    "        while (end < 512 and BIO_tagging[end] == 1):\n",
    "          end += 1\n",
    "        if (end > start + 1):\n",
    "          if (start <= 3):\n",
    "            s = tokenizer.decode(token_ids = tokens_tensors[start : end +2], skip_special_tokens = True)\n",
    "          else:\n",
    "            s = tokenizer.decode(token_ids = tokens_tensors[start-1 : end +2], skip_special_tokens = True)\n",
    "          s = s.replace(' ', '')\n",
    "          # print(s)\n",
    "          for k in range(len(ckip_result)):\n",
    "            if (len(ckip_result[k]) < 2):\n",
    "              continue\n",
    "            elif (re.findall(r\"[%s]+\" %non_stops, ckip_result[k]) != [] \\\n",
    "                     or re.findall(r\"[%s]+\" %stops, ckip_result[k]) != []): # 有標點\n",
    "              continue\n",
    "            found = s.find(ckip_result[k])\n",
    "            if (found != -1):\n",
    "              if (found == 0):\n",
    "                # print(s)\n",
    "                prob = BIO_prob[start][0] # begin\n",
    "                for i in range(1, len(ckip_result[k]) + 1):\n",
    "                  p = BIO_prob[start+i][1]  # inside\n",
    "                  # print(p)\n",
    "                  prob *= p\n",
    "                # print('! len: ', len(ckip_result[k]), '\\tprobability: ', prob.item())\n",
    "              else:\n",
    "                # print(s)\n",
    "                prob = BIO_prob[start][1] # inside\n",
    "                for i in range(1, len(ckip_result[k]) + 1):\n",
    "                  p = BIO_prob[start+i][1]  # inside\n",
    "                  # print(p)\n",
    "                  prob *= p\n",
    "                # print('_ len: ', len(ckip_result[k]), '\\tprobability: ', prob.item())\n",
    "#               if (prob.item() >= 0.95):\n",
    "              if (True):\n",
    "                result.append(ckip_result[k])\n",
    "    if (len(result) == 0):\n",
    "      result.append('')\n",
    "    # print('---')\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zaeoDS0np-0S"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, testLoader, BATCH_SIZE):\n",
    "  result = []\n",
    "  total_count = 0 # 第n筆data\n",
    "  with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "      # 將所有 tensors 移到 GPU 上\n",
    "      if next(model.parameters()).is_cuda:\n",
    "        data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "      \n",
    "      # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "      # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "      tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "      outputs = model(input_ids=tokens_tensors, \n",
    "                  token_type_ids=segments_tensors, \n",
    "                  attention_mask=masks_tensors)\n",
    "      \n",
    "      count = min(outputs[0].shape[0], BATCH_SIZE)\n",
    "      for i in range(count):  # run batchsize times\n",
    "#         have_AML = outputs[0][i].argmax()\n",
    "        BIO_pred = outputs[0][i].argmax(1) # 3*512 into class label\n",
    "        text_token = tokens_tensors[i]\n",
    "        ckip_names = df_test.loc[total_count, 'ckip_names']\n",
    "        ckip_names_list = ast.literal_eval(ckip_names) # string to list\n",
    "        r = bio_2_string(text_token, test_binary_y[total_count], BIO_pred, ckip_names_list, outputs[0][i])\n",
    "#         print(BIO_pred)\n",
    "        result.append(r)\n",
    "        total_count += 1\n",
    "#       break\n",
    "    # print(result)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkAJDq2Qt72t"
   },
   "outputs": [],
   "source": [
    "\"\"\"testing\"\"\"\n",
    "import torch\n",
    "from transformers import BertConfig\n",
    "config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME, output_hidden_states=True)\n",
    "# model = AMLPredictModel(config)\n",
    "# model.load_state_dict(torch.load(MODEL_PATH))\n",
    "# model = torch.load(MODEL_PATH)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "testSet = TestDataset(test_input_dict)\n",
    "testLoader = DataLoader(testSet, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "predictions = get_predictions(model, testLoader, BATCH_SIZE)\n",
    "\n",
    "# pred = predictions.cpu().data.numpy()\n",
    "# pred = np.argmax(pred, axis=1)\n",
    "# accuracy = (pred == test_binary_y).mean()\n",
    "# print('Your test accuracy is %.6f' % (accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get answer and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1595407695986,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "ATDm-9ZGqnOB",
    "outputId": "8b657889-faec-4efc-bd75-926de13ec499"
   },
   "outputs": [],
   "source": [
    "temp = df_test['name'].tolist()\n",
    "ans = []\n",
    "for i in range(len(temp)):\n",
    "  t = ast.literal_eval(temp[i])\n",
    "  if (len(t) == 0):\n",
    "    t.append('')\n",
    "  ans.append(t)\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p5uEYssCYfD"
   },
   "outputs": [],
   "source": [
    "def eval(pred, ans):\n",
    "    if bool(pred) is not bool(ans):\n",
    "        return 0\n",
    "    elif not pred and not ans:\n",
    "        return 1\n",
    "    else:\n",
    "        pred = set(pred)\n",
    "        ans = set(ans)\n",
    "        interaction_len = len(pred & ans)\n",
    "        if interaction_len == 0:\n",
    "            return 0\n",
    "\n",
    "        pred_len = len(pred)\n",
    "        ans_len = len(ans)\n",
    "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
    "\n",
    "\n",
    "def eval_all(pred_list, ans_list):\n",
    "    assert len(pred_list) == len(ans_list)\n",
    "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### name classfier by Mouth Han"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l360S9dPlOLP"
   },
   "outputs": [],
   "source": [
    "# df_ckip = pd.read_csv('./ckip/ckip_dataset3.csv')\n",
    "df_ckip = pd.read_csv('./ckip/ckip.csv')\n",
    "ckip_name = df_ckip.loc[df_ckip['ans'] == 1, 'name'].tolist()\n",
    "# ckip_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N69stFCSkVp-"
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "ckip_name = set(ckip_name)\n",
    "for i in range(len(predictions)):\n",
    "  temp = set(predictions[i])\n",
    "  r = list(ckip_name & temp)\n",
    "  if (len(r) == 0):\n",
    "    r.append('')\n",
    "  result.append(r)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875569779846766"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_all(result, ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### name classifier (QA model) by Houg Yun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "name classifier (QA model) by Houg Yun\n",
    "input: predicted name (list), news(text), dataset num(int)\n",
    "output: predict name (list)\n",
    "model has been delete QAQ\n",
    "\"\"\"\n",
    "def check_pred_name_is_real_ans(pred_name_list,news,dataset):\n",
    "    class Testset(Dataset):\n",
    "        def __init__(self, input_ids , token_type_ids , attention_mask):\n",
    "            self.input_ids = input_ids\n",
    "            self.token_type_ids = token_type_ids\n",
    "            self.attention_mask = attention_mask\n",
    "        def __getitem__(self,idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "\n",
    "            return inputid , tokentype , attentionmask\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    \n",
    "    lm_path = './bert_wwm_pretrain_tbrain/'\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "    content = clean_string(news)\n",
    "    train_input_ids = []\n",
    "    train_token_types = []\n",
    "    train_attention_mask = []\n",
    "        \n",
    "    for name in pred_name_list:\n",
    "        \n",
    "        content_max_length = 512-3-len(name)\n",
    "        \n",
    "        if len(content) >= content_max_length:\n",
    "            content = content[:content_max_length]\n",
    "            \n",
    "        input_ids = tokenizer.encode(name, content)\n",
    "        if(len(input_ids)>512):\n",
    "            continue\n",
    "        sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "        num_seg_a = sep_index + 1\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        while len(input_ids) < 512:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            \n",
    "        train_input_ids.append(input_ids)\n",
    "        train_token_types.append(segment_ids)\n",
    "        train_attention_mask.append(input_mask)\n",
    "        \n",
    "    train_input_ids = np.array(train_input_ids)\n",
    "    train_token_types  = np.array(train_token_types)\n",
    "    train_attention_mask = np.array(train_attention_mask)\n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = train_input_ids.shape[0]\n",
    "    \n",
    "    testset = Testset(train_input_ids ,train_token_types , train_attention_mask)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    from transformers import BertForSequenceClassification\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    lm_path = './bert_wwm_pretrain_tbrain/'\n",
    "    NUM_LABELS = 2\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "    \n",
    "    check_point = ''\n",
    "    if(dataset == 0):\n",
    "#         最一開始的dataset\n",
    "        check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_13.pkl'\n",
    "\n",
    "    elif (dataset == 1):\n",
    "#         tbrain_train (1).csv\n",
    "        check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_dataset1_epoch17.pkl'\n",
    "\n",
    "    elif (dataset == 2):\n",
    "#         tbrain_train (2).csv\n",
    "        check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_dataset2_epoch13.pkl'\n",
    "\n",
    "    elif (dataset == 3):\n",
    "#         tbrain_train (3).csv\n",
    "        check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_dataset3_epoch18.pkl'\n",
    "\n",
    "    elif (dataset == 4):\n",
    "#        traindata + testdata\n",
    "        check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_alldataset_epoch18.pkl'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_13.pkl'\n",
    "    model.load_state_dict(torch.load(check_point))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "            pred = torch.softmax(outputs[0] , dim = -1)\n",
    "            torch.set_printoptions(precision=10)\n",
    "            print(pred)\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            pred_name_list = np.array(pred_name_list)\n",
    "            return list(pred_name_list[pred>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(ans)):\n",
    "    if (predictions[i][0] == ''):\n",
    "        result.append([''])\n",
    "        continue\n",
    "    result.append(check_pred_name_is_real_ans(predictions[i] , df_test.loc[i,'full_content'], 1))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17219,
     "status": "ok",
     "timestamp": 1595407726669,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "l9mHMcv91BSH",
    "outputId": "c4fc8334-5956-476f-8755-bfc05fff0426"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9897097245366087"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_all(result, ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Only name testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8348777348777349\n"
     ]
    }
   ],
   "source": [
    "only_name_ans = []\n",
    "only_name_pred = []\n",
    "for i in range(len(ans)):\n",
    "    if (ans[i][0] != ''):\n",
    "        only_name_ans.append(ans[i])\n",
    "        only_name_pred.append(result[i])\n",
    "print(eval_all(only_name_pred, only_name_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_bert_wwm_result1 = [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['賴俊吉'], [''], [''], ['張銘坤', '陳揚宗'], [''], [''], [''], [''], [''], [''], ['王益洲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李士綸', '吳哲瑋'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林志聰', '伍政山'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['張承平'], [''], [''], [''], [''], [''], [''], ['雷俊玲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳仕修', '黃國昌', '徐仲榮'], [''], ['林睿耆', '周漢祥', '林昱伯', '詹騏瑋', '林煒智'], [''], [''], ['秦儷舫', '童仲彥', '黃國昌'], [''], [''], [''], [''], ['蘇怡寧', '禾馨'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['詹舜淇', '方俐婷', '詹逸宏', '詹雅琳', '詹雯婷'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['黃顯雄', '黃世陽'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['葉大慧', '吳國昌', '吳孝昌', '魏君婷', '張欽堯'], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李宛霞', '黃梅雪'], ['陳有意', '陳運作', '陳坦承', '陳武騰', '黃振榮'], ['許長裕'], [''], [''], ['蔡開宇', '李訓成', '王宇正'], [''], [''], [''], ['蔡思庭', '楊正平'], [''], [''], [''], [''], [''], [''], [''], ['林勇任', '郭雅雯', '蘇震清', '葉美麗', '茂宇', '賴麗團'], [''], ['王宇承', '陳瑞芳', '振瑞', '陳澤信'], [''], [''], ['李育英', '林煜傑', '李文潔', '劉矢口'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['崔明禮', '楊敬熙'], [''], [''], ['黃聲儀', '陳功源', '黃泳學', '羅栩亮', '黃馨儀', '高兆良'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['洪正義'], [''], [''], [''], [''], [''], [''], ['鍾增林', '曾國財'], [''], ['王毓雅', '羅瑞榮'], [''], [''], [''], [''], [''], ['蔡文娟'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['柯賜海'], [''], [''], [''], [''], ['黃文鴻', '吳承霖', '陳玟叡', '蔡英俊'], [''], [''], [''], [''], [''], ['賴素如'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['謝宥宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['郭政權', '楊天生', '蔡茂寅', '郭說明'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['吳承霖', '陳玟叡'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['何壽川'], [''], [''], [''], [''], [''], [''], [''], [''], ['李錫璋', '陳清江'], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
    "\n",
    "pre_bert_wwm_result2 = [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['宋芷妍', '王安石'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['繆竹怡'], [''], [''], ['陳建飛'], [''], [''], ['許玉秀', '吳淑珍', '王隆昌'], [''], [''], ['劉威甫'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['邱嘉進'], [''], [''], [''], ['陳宣銘'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['許家榮', '林詩芳', '王紀棠', '殷倜凡', '謝巧莉'], [''], [''], ['姚慶佳', '姚慶佳男'], ['吳運豐', '雲從龍'], ['李春生', '周正華', '黃建強', '高振殷', '鄭銘富', '呂家緯'], [''], [''], [''], [''], [''], [''], ['鄧超鴻', '道克明'], [''], [''], [''], [''], [''], [''], [''], [''], ['崔明禮'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡賜爵', '劉昌松', '畢鈞輝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳韋霖'], [''], [''], [''], [''], ['柯志龍', '俞小凡', '翁家明', '瓊瑤', '張興蕙', '張哲維', '夏婉君'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蒲念慈'], [''], ['詹舜淇', '詹逸宏', '詹雅琳'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['侯君鍵'], [''], [''], [''], [''], ['李孟謙', '于曉艷'], [''], [''], [''], [''], [''], [''], ['楚瑞芳', '王光遠', '錢利忠'], [''], [''], [''], [''], [''], ['鍾增林', '曾國財'], [''], [''], [''], [''], [''], ['李榮華'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楚瑞芳', '王光遠', '鍾榮昌', '彭振源'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['賴嚮景', '陳俊宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李佰全', '葉玲', '林陀桂英'], [''], [''], [''], [''], ['林政賢'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楊治渝', '李宗瑞'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['吳東明', '劉吉雄', '林輝宏', '張建華', '裴振福', '呂宗南'], [''], [''], ['葉冠廷', '康明璋'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['何培才'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林金龍', '黃鈺蘋', '呂翠峰', '黃子愛', '顏雪藝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳建三', '羅秋英', '陳斯婷', '陳斯婷批戰袍'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['王益洲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['黃振龍', '蔡文旭', '張治忠', '張道銘'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
    "\n",
    "pre_bert_wwm_result3 = [[''], [''], [''], [''], [''], [''], ['張君豪', '李孟謙', '于曉艷'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['崔培明'], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡賜爵', '劉昌松', '畢鈞輝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳韋霖'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林欣月'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['黃薪哲', '吳寶玉', '余信憲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['袁昶平', '洪勝明'], [''], [''], ['葉麗珍', '祥禾', '趙鈞震', '葉麗貞', '陳耀東'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林愈得', '陳清裕', '曾盛陽', '曾盛麟', '曾美菁'], [''], [''], [''], [''], [''], [''], [''], ['林勇任', '郭雅雯', '葉美麗', '茂宇', '賴麗團'], [''], [''], [''], ['蔡維峻', '林銘宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楊嘉仁', '林崇傑'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楊昇穎', '林嘉東'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['卓國華'], [''], [''], [''], ['戴盛世', '宣昶孔'], [''], ['許祈文'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李維凱'], [''], [''], [''], [''], ['吳宗憲', '林清井', '湯蕙禎', '劉奕發', '歐炳辰'], ['陳發貴'], [''], [''], [''], [''], [''], [''], ['陳俊佑', '陳致銘', '王延順'], [''], [''], [''], ['孔朝'], [''], ['詹昭書'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['朱啟仁'], ['戴盛世'], [''], [''], [''], ['徐詩彥'], [''], [''], [''], [''], ['陳麗珍'], [''], [''], [''], [''], ['何培才'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['許玉秀', '吳淑珍', '王隆昌'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡思庭', '楊正平'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['游美雲'], [''], [''], [''], [''], ['洪丞俊', '謝介裕', '黃丹怡'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['范筱梵夫', '江智銓', '江智詮', '王柏森', '范筱梵'], [''], [''], [''], [''], [''], [''], ['吳銀嵐', '吳金虎', '張安樂', '徐宏杰', '許國楨'], [''], [''], [''], ['蒲念慈'], ['陳之漢', '紀雅玲', '林睿君'], [''], ['李榮勝', '黃錦燕'], [''], [''], ['']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBTL3_result1 = [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['賴俊吉'], [''], [''], ['張銘坤', '陳揚宗'], [''], [''], [''], [''], [''], [''], ['王益洲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李士綸', '吳哲瑋'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林志聰', '伍政山'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['張承平'], [''], [''], [''], [''], [''], [''], ['雷俊玲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡英文', '陳仕修', '黃國昌', '徐仲榮'], [''], ['林睿耆', '周漢祥', '林昱伯', '詹騏瑋', '林煒智'], [''], [''], ['秦儷舫', '童仲彥', '黃國昌'], [''], [''], [''], [''], ['禾馨'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['詹舜淇', '方俐婷', '詹逸宏', '詹雅琳', '詹雯婷'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['黃顯雄', '黃世陽'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['葉大慧', '吳國昌', '吳孝昌', '魏君婷', '張欽堯'], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李宛霞', '黃梅雪'], ['陳運作', '陳坦承', '黃振榮', '陳武騰', '黃不滿'], ['許長裕'], [''], [''], ['蔡開宇', '李訓成', '王宇正'], [''], [''], [''], ['蔡思庭', '楊正平'], [''], [''], [''], [''], [''], [''], [''], ['林勇任', '郭雅雯', '蘇震清', '葉美麗', '茂宇', '賴麗團'], [''], ['王俊忠', '王宇承', '振瑞', '陳澤信'], [''], [''], ['李育英', '林煜傑', '李文潔', '劉矢口'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['崔明禮', '楊敬熙'], [''], [''], ['黃聲儀', '陳功源', '黃泳學', '羅栩亮', '黃馨儀'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['洪正義'], [''], [''], [''], [''], [''], [''], ['鍾增林', '曾國財'], [''], ['王毓雅', '羅瑞榮'], [''], [''], [''], [''], [''], ['蔡文娟'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['柯賜海'], [''], [''], [''], [''], ['黃文鴻', '吳承霖', '丁偉杰', '陳玟叡', '蔡英俊'], [''], [''], [''], [''], [''], ['賴素如'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['謝宥宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['郭政權', '楊天生', '蔡茂寅', '郭說明'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['吳承霖', '陳玟叡'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['何壽川'], [''], [''], [''], [''], [''], [''], [''], [''], ['李錫璋', '陳清江'], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
    "\n",
    "RBTL3_result2 = [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['王安石'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['繆竹怡'], [''], [''], ['陳建飛'], [''], [''], ['許玉秀', '吳淑珍', '王隆昌'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['邱嘉進', '游芳男'], [''], [''], [''], ['陳宣銘'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['許家榮', '林詩芳', '林維俊', '王紀棠', '殷倜凡', '謝巧莉'], [''], [''], ['姚慶佳', '姚慶佳男'], ['吳運豐', '雲從龍'], ['李春生', '周正華', '黃建強', '高振殷', '鄭銘富', '呂家緯'], [''], [''], [''], [''], [''], [''], ['鄧超鴻', '道克明'], [''], [''], [''], [''], [''], [''], [''], [''], ['崔明禮'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡賜爵', '劉昌松', '畢鈞輝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳韋霖'], [''], [''], [''], [''], ['柯志龍', '俞小凡', '瓊瑤', '林俊峰', '張興蕙', '張哲維', '夏婉君'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蒲念慈'], [''], ['詹雯婷', '方俐婷', '詹逸宏', '詹雅琳'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['侯君鍵'], [''], [''], [''], [''], ['于曉艷'], [''], [''], [''], [''], [''], [''], ['楚瑞芳', '王光遠'], [''], [''], [''], [''], [''], ['鍾增林', '曾國財'], [''], [''], [''], [''], [''], ['李榮華', '鄭徒刑'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楚瑞芳', '彭振源'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['賴嚮景', '陳俊宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李佰全', '葉玲', '林陀桂英'], [''], [''], [''], [''], ['林政賢'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楊治渝', '李宗瑞'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['吳東明', '劉吉雄', '林輝宏', '張建華', '裴振福'], [''], [''], ['陳菊', '葉冠廷', '康明璋'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['何培才'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['顏則獲', '林金龍', '黃鈺蘋', '黃子愛', '顏雪藝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳建三', '羅秋英'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['王益洲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡文旭', '張道銘', '呂雅純', '張治忠', '蔡英俊', '黃振龍', '李欣潔'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
    "\n",
    "RBTL3_result3 = [[''], [''], [''], [''], [''], [''], ['李孟謙', '于曉艷'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['崔培明'], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡賜爵', '劉昌松', '畢鈞輝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳韋霖'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林欣月'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['黃薪哲', '吳寶玉', '余信憲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['袁昶平', '洪勝明', '洪介紹'], [''], [''], ['祥禾', '葉麗珍', '葉麗貞', '趙鈞震'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳清裕', '林愈得', '曾美菁', '曾盛麟'], [''], [''], [''], [''], [''], [''], [''], ['林勇任', '郭雅雯', '蘇震清', '葉美麗', '茂宇', '賴麗團'], [''], [''], [''], ['蔡維峻', '林銘宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林崇傑'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['周宗賢'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['許祈文'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李發布', '李維凱'], [''], [''], [''], [''], ['劉奕發', '吳宗憲', '歐炳辰', '林清井'], ['陳發貴'], [''], [''], [''], [''], [''], [''], ['陳俊佑', '陳致銘', '王延順'], [''], [''], [''], ['孔朝'], [''], ['詹昭書', '洪美秀'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['朱啟仁', '楊善淵'], ['楊富巖', '戴盛世', '錢利忠'], [''], [''], [''], ['徐詩彥', '林繼蘇'], [''], [''], [''], [''], ['陳麗珍'], [''], [''], [''], [''], ['何培才'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['許玉秀', '吳淑珍', '王隆昌'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡思庭', '楊正平'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['游美雲'], [''], [''], [''], [''], ['洪丞俊', '謝介裕', '黃丹怡'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['王柏森', '江智銓', '范筱梵', '江智詮'], [''], [''], [''], [''], [''], [''], ['徐宏杰', '許國楨', '吳金虎', '張安樂'], [''], [''], [''], [''], ['林睿君'], [''], ['李榮勝', '黃錦燕'], [''], [''], ['']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9778375844567293\n",
      "0.9698493518249118\n"
     ]
    }
   ],
   "source": [
    "union_result = []\n",
    "intersect_result = []\n",
    "for i in range(len(ans)):\n",
    "    temp1 = set(pre_bert_wwm_result3[i])\n",
    "    temp2 = set(RBTL3_result3[i])\n",
    "    union = list(temp1 | temp2)\n",
    "    intersect = list(temp1 & temp2)\n",
    "    if (len(union) == 0):\n",
    "        union.append('')\n",
    "    if (len(intersect) == 0):\n",
    "        intersect.append('')\n",
    "\n",
    "    union_result.append(union)\n",
    "    intersect_result.append(intersect)\n",
    "print(eval_all(union_result,ans))\n",
    "print(eval_all(intersect_result,ans))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AML_RBTL3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
